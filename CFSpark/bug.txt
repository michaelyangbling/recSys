log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /stderr (Permission denied)
 at java.io.FileOutputStream.open0(Native Method)
 at java.io.FileOutputStream.open(FileOutputStream.java:270)
 at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
 at java.io.FileOutputStream.<init>(FileOutputStream.java:133)
 at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
 at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
 at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
 at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
 at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
 at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
 at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
 at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
 at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
 at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
 at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
 at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
 at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
 at org.apache.spark.internal.Logging$class.initializeLogging(Logging.scala:120)
 at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:108)
 at org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:71)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:79)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /stdout (Permission denied)
 at java.io.FileOutputStream.open0(Native Method)
 at java.io.FileOutputStream.open(FileOutputStream.java:270)
 at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
 at java.io.FileOutputStream.<init>(FileOutputStream.java:133)
 at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
 at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
 at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
 at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
 at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
 at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
 at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
 at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
 at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)
 at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
 at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
 at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
 at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
 at org.apache.spark.internal.Logging$class.initializeLogging(Logging.scala:120)
 at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:108)
 at org.apache.spark.deploy.SparkSubmit.initializeLogIfNecessary(SparkSubmit.scala:71)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:79)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stdout].
19/04/16 00:20:35 WARN DependencyUtils: Skip remote jar s3://ds5230-spark/CF.jar.
19/04/16 00:20:36 INFO RMProxy: Connecting to ResourceManager at ip-172-31-85-163.ec2.internal/172.31.85.163:8032
19/04/16 00:20:36 INFO Client: Requesting a new application from cluster with 5 NodeManagers
19/04/16 00:20:36 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
19/04/16 00:20:36 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
19/04/16 00:20:36 INFO Client: Setting up container launch context for our AM
19/04/16 00:20:36 INFO Client: Setting up the launch environment for our AM container
19/04/16 00:20:36 INFO Client: Preparing resources for our AM container
19/04/16 00:20:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/04/16 00:20:38 INFO Client: Uploading resource file:/mnt/tmp/spark-1c570d5e-375d-412c-a7c5-a0291d2c4714/__spark_libs__4729293676082435012.zip -> hdfs://ip-172-31-85-163.ec2.internal:8020/user/hadoop/.sparkStaging/application_1555373900940_0001/__spark_libs__4729293676082435012.zip
19/04/16 00:20:41 INFO Client: Uploading resource s3://ds5230-spark/CF.jar -> hdfs://ip-172-31-85-163.ec2.internal:8020/user/hadoop/.sparkStaging/application_1555373900940_0001/CF.jar
19/04/16 00:20:41 INFO S3NativeFileSystem: Opening 's3://ds5230-spark/CF.jar' for reading
19/04/16 00:20:41 INFO Client: Uploading resource file:/mnt/tmp/spark-1c570d5e-375d-412c-a7c5-a0291d2c4714/__spark_conf__6941528392873772362.zip -> hdfs://ip-172-31-85-163.ec2.internal:8020/user/hadoop/.sparkStaging/application_1555373900940_0001/__spark_conf__.zip
19/04/16 00:20:42 INFO SecurityManager: Changing view acls to: hadoop
19/04/16 00:20:42 INFO SecurityManager: Changing modify acls to: hadoop
19/04/16 00:20:42 INFO SecurityManager: Changing view acls groups to:
19/04/16 00:20:42 INFO SecurityManager: Changing modify acls groups to:
19/04/16 00:20:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
19/04/16 00:20:43 INFO Client: Submitting application application_1555373900940_0001 to ResourceManager
19/04/16 00:20:44 INFO YarnClientImpl: Submitted application application_1555373900940_0001
19/04/16 00:20:45 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:45 INFO Client:
  client token: N/A
  diagnostics: AM container is launched, waiting for AM container to Register with RM
  ApplicationMaster host: N/A
  ApplicationMaster RPC port: -1
  queue: default
  start time: 1555374043769
  final status: UNDEFINED
  tracking URL: http://ip-172-31-85-163.ec2.internal:20888/proxy/application_1555373900940_0001/
  user: hadoop
19/04/16 00:20:46 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:47 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:48 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:49 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:50 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:51 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:52 INFO Client: Application report for application_1555373900940_0001 (state: RUNNING)
19/04/16 00:20:52 INFO Client:
  client token: N/A
  diagnostics: N/A
  ApplicationMaster host: ip-172-31-91-110.ec2.internal
  ApplicationMaster RPC port: 36993
  queue: default
  start time: 1555374043769
  final status: UNDEFINED
  tracking URL: http://ip-172-31-85-163.ec2.internal:20888/proxy/application_1555373900940_0001/
  user: hadoop
19/04/16 00:20:53 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:53 INFO Client:
  client token: N/A
  diagnostics: [Tue Apr 16 00:20:52 +0000 2019] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = CORE ; Partition Resource = <memory:61440, vCores:40> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 0.0 % ; Queue's Absolute max capacity = 100.0 % ;
  ApplicationMaster host: N/A
  ApplicationMaster RPC port: -1
  queue: default
  start time: 1555374043769
  final status: UNDEFINED
  tracking URL: http://ip-172-31-85-163.ec2.internal:20888/proxy/application_1555373900940_0001/
  user: hadoop
19/04/16 00:20:54 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:55 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:56 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:57 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:58 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:20:59 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:21:00 INFO Client: Application report for application_1555373900940_0001 (state: ACCEPTED)
19/04/16 00:21:01 INFO Client: Application report for application_1555373900940_0001 (state: RUNNING)
19/04/16 00:21:01 INFO Client:
  client token: N/A
  diagnostics: N/A
  ApplicationMaster host: ip-172-31-86-140.ec2.internal
  ApplicationMaster RPC port: 37489
  queue: default
  start time: 1555374043769
  final status: UNDEFINED
  tracking URL: http://ip-172-31-85-163.ec2.internal:20888/proxy/application_1555373900940_0001/
  user: hadoop
19/04/16 00:21:02 INFO Client: Application report for application_1555373900940_0001 (state: FINISHED)
19/04/16 00:21:02 INFO Client:
  client token: N/A
  diagnostics: User class threw exception: java.io.FileNotFoundException: s3:/ds5230-spark/bigTrain.txt (No such file or directory)
 at java.io.FileInputStream.open0(Native Method)
 at java.io.FileInputStream.open(FileInputStream.java:195)
 at java.io.FileInputStream.<init>(FileInputStream.java:138)
 at scala.io.Source$.fromFile(Source.scala:91)
 at scala.io.Source$.fromFile(Source.scala:76)
 at scala.io.Source$.fromFile(Source.scala:54)
 at project.CF$.updateMap(CF.scala:22)
 at project.CF$.main(CF.scala:73)
 at project.CF.main(CF.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)

  ApplicationMaster host: ip-172-31-86-140.ec2.internal
  ApplicationMaster RPC port: 37489
  queue: default
  start time: 1555374043769
  final status: FAILED
  tracking URL: http://ip-172-31-85-163.ec2.internal:20888/proxy/application_1555373900940_0001/
  user: hadoop
19/04/16 00:21:02 ERROR Client: Application diagnostics message: User class threw exception: java.io.FileNotFoundException: s3:/ds5230-spark/bigTrain.txt (No such file or directory)
 at java.io.FileInputStream.open0(Native Method)
 at java.io.FileInputStream.open(FileInputStream.java:195)
 at java.io.FileInputStream.<init>(FileInputStream.java:138)
 at scala.io.Source$.fromFile(Source.scala:91)
 at scala.io.Source$.fromFile(Source.scala:76)
 at scala.io.Source$.fromFile(Source.scala:54)
 at project.CF$.updateMap(CF.scala:22)
 at project.CF$.main(CF.scala:73)
 at project.CF.main(CF.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)

Exception in thread "main" org.apache.spark.SparkException: Application application_1555373900940_0001 finished with failed status
 at org.apache.spark.deploy.yarn.Client.run(Client.scala:1149)
 at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
19/04/16 00:21:02 INFO ShutdownHookManager: Shutdown hook called
19/04/16 00:21:02 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-1c570d5e-375d-412c-a7c5-a0291d2c4714
19/04/16 00:21:02 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-69e7b7ea-503a-4bcb-a3d9-3d4f21dd1e26
Command exiting with ret '1'